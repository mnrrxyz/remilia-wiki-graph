import requests
import json
import time
from collections import defaultdict

# ==================== FILTROS CONFIGURABLES ====================

# Prefijos de p√°ginas a EXCLUIR completamente
EXCLUDE_PREFIXES = [
    'Category:',
    'File:',
    'Template:',
    'Template talk:',
    'Special:',
    'Help:',
    'MediaWiki:',
    'User:',
    'User talk:',
    'Talk:',
    'Wikipedia:',
]

# Palabras clave que indican p√°ginas de navegaci√≥n/sistema a EXCLUIR
EXCLUDE_KEYWORDS = [
    'navigation',
    'Navigation',
]

# ==================== API FUNCTIONS ====================

def get_all_wiki_pages():
    """Obtiene lista de TODAS las p√°ginas de la wiki (main namespace)"""
    url = "https://wiki.remilia.org/api.php"
    
    all_pages = []
    continue_param = {}
    
    print("üîç Descubriendo todas las p√°ginas de la wiki...")
    
    while True:
        params = {
            'action': 'query',
            'list': 'allpages',
            'apnamespace': 0,  # Solo main namespace
            'aplimit': 500,
            'format': 'json',
            **continue_param
        }
        
        response = requests.get(url, params=params)
        data = response.json()
        
        pages = data.get('query', {}).get('allpages', [])
        for page in pages:
            all_pages.append(page['title'])
        
        print(f"  Descubiertas: {len(all_pages)} p√°ginas...")
        
        if 'continue' in data:
            continue_param = data['continue']
        else:
            break
    
    print(f"‚úÖ Total de p√°ginas encontradas: {len(all_pages)}\n")
    return all_pages

def get_page_links_api(page_title):
    """Obtiene todos los links de una p√°gina usando la API"""
    url = "https://wiki.remilia.org/api.php"
    
    links = []
    continue_param = {}
    
    while True:
        params = {
            'action': 'query',
            'titles': page_title,
            'prop': 'links',
            'pllimit': 500,
            'format': 'json',
            **continue_param
        }
        
        try:
            response = requests.get(url, params=params, timeout=10)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                if 'links' in page_data:
                    for link in page_data['links']:
                        links.append(link['title'])
            
            if 'continue' in data:
                continue_param = data['continue']
            else:
                break
        except Exception as e:
            print(f"  ‚ùå API error: {e}")
            break
    
    return links

def filter_links(links, verbose=False):
    """
    Filtra links seg√∫n las reglas configurables
    
    EXCLUYE:
    - P√°ginas de navegaci√≥n (templates, categor√≠as, etc)
    - Links externos (estos vienen en otra prop de la API)
    - P√°ginas de sistema/ayuda
    
    INCLUYE:
    - Todo el contenido conceptual (incluyendo See Also)
    - Links a otras p√°ginas de la wiki en main namespace
    """
    filtered = []
    excluded = []
    
    for link in links:
        # Check prefijos
        if any(link.startswith(prefix) for prefix in EXCLUDE_PREFIXES):
            excluded.append((link, 'prefix'))
            continue
        
        # Check keywords
        if any(keyword in link for keyword in EXCLUDE_KEYWORDS):
            excluded.append((link, 'keyword'))
            continue
        
        # Si pas√≥ todos los filtros, incluir
        filtered.append(link)
    
    if verbose and excluded:
        print(f"    Excluidos ({len(excluded)}):")
        for link, reason in excluded[:3]:
            print(f"      - {link} (raz√≥n: {reason})")
        if len(excluded) > 3:
            print(f"      ... y {len(excluded) - 3} m√°s")
    
    return filtered

# ==================== CRAWLER ====================

def crawl_wiki(pages=None, verbose=True):
    """Crawlea la wiki con API + filtros configurables"""
    
    if pages is None:
        pages = get_all_wiki_pages()
    
    graph = {}
    stats = {
        'total_pages': len(pages),
        'total_raw_links': 0,
        'total_filtered_links': 0,
        'pages_with_links': 0,
        'pages_without_links': 0
    }
    
    print(f"üöÄ Crawleando {len(pages)} p√°ginas...\n")
    
    for i, page in enumerate(pages, 1):
        if verbose:
            print(f"[{i}/{len(pages)}] {page}")
        
        # Obtener links raw de la API
        raw_links = get_page_links_api(page)
        stats['total_raw_links'] += len(raw_links)
        
        # Filtrar seg√∫n configuraci√≥n
        filtered_links = filter_links(raw_links, verbose=verbose)
        stats['total_filtered_links'] += len(filtered_links)
        
        graph[page] = filtered_links
        
        if filtered_links:
            stats['pages_with_links'] += 1
        else:
            stats['pages_without_links'] += 1
        
        if verbose:
            print(f"  ‚îî‚îÄ {len(raw_links)} raw ‚Üí {len(filtered_links)} filtrados\n")
        
        time.sleep(0.3)  # Rate limiting
    
    return graph, stats

# ==================== ANALYSIS ====================

def analyze_graph(graph, stats):
    """An√°lisis del grafo final"""
    
    print("\n" + "="*60)
    print("üìä AN√ÅLISIS DEL GRAFO")
    print("="*60)
    
    print(f"\nüìà Estad√≠sticas generales:")
    print(f"  Total de p√°ginas: {stats['total_pages']}")
    print(f"  P√°ginas con links: {stats['pages_with_links']}")
    print(f"  P√°ginas sin links: {stats['pages_without_links']}")
    print(f"  Links raw (API): {stats['total_raw_links']}")
    print(f"  Links filtrados: {stats['total_filtered_links']}")
    print(f"  Reducci√≥n: {((stats['total_raw_links'] - stats['total_filtered_links']) / stats['total_raw_links'] * 100):.1f}%")
    
    avg_links = stats['total_filtered_links'] / stats['total_pages']
    print(f"  Promedio de links por p√°gina: {avg_links:.1f}")
    
    # P√°ginas m√°s referenciadas (centralidad)
    incoming = defaultdict(int)
    for source, targets in graph.items():
        for target in targets:
            incoming[target] += 1
    
    print(f"\nüîó Top 10 conceptos m√°s referenciados:")
    for page, count in sorted(incoming.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {count:3d} ‚Üê {page}")
    
    # P√°ginas con m√°s links salientes
    print(f"\nüì§ Top 10 p√°ginas con m√°s conexiones:")
    outgoing = [(page, len(links)) for page, links in graph.items()]
    for page, count in sorted(outgoing, key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {count:3d} ‚Üí {page}")
    
    # P√°ginas aisladas (sin links entrantes ni salientes)
    isolated = [
        page for page in graph.keys() 
        if len(graph[page]) == 0 and incoming.get(page, 0) == 0
    ]
    if isolated:
        print(f"\n‚ö†Ô∏è  P√°ginas aisladas (sin conexiones): {len(isolated)}")
        for page in isolated[:5]:
            print(f"    - {page}")
        if len(isolated) > 5:
            print(f"    ... y {len(isolated) - 5} m√°s")

def export_for_visualization(graph, filename='remilia_graph_final.json'):
    """Exporta en formato listo para visualizaci√≥n"""
    
    # Formato con metadata √∫til
    export = {
        'metadata': {
            'total_nodes': len(graph),
            'total_edges': sum(len(links) for links in graph.values()),
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        },
        'graph': graph
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(export, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ Grafo exportado a: {filename}")

# ==================== MAIN ====================

if __name__ == "__main__":
    
    print("üåê REMILIA WIKI GRAPH CRAWLER")
    print("="*60)
    print(f"\nFiltros activos:")
    print(f"  Excluir prefijos: {', '.join(EXCLUDE_PREFIXES[:5])}...")
    print(f"  Excluir keywords: {', '.join(EXCLUDE_KEYWORDS)}")
    print()
    
    # Crawlear
    graph, stats = crawl_wiki(verbose=True)
    
    # Analizar
    analyze_graph(graph, stats)
    
    # Exportar
    export_for_visualization(graph)
    
    print("\n‚ú® Done!")